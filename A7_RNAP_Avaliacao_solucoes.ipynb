{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RNAP-07-Avaliacao_solucoes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K8CDQUj8yqpq"},"source":["## MBA em Ciência de Dados\n","# Redes Neurais e Arquiteturas Profundas\n","\n","### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n","\n","\n","### <span style=\"color:darkred\">Avaliação (com soluções)</span>\n","\n","Moacir Antonelli Ponti\n","\n","CeMEAI - ICMC/USP São Carlos\n","\n","---\n","\n","As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"xMJ4IFd7yqpt"},"source":["### Questão 1)\n","\n","Qual das alternativas melhor descreve o objetivo de um modelo de aprendizado por reforço?\n","\n","(a) Aprender a predizer o próximo estado com base em uma ação atual em determinado ambiente<br>\n","(b) Aprender a agrupar ações com base na recompensa que oferecem<br>\n","(c) Aprender uma política que permita obter agentes que mapeiem o ambiente de forma a minimizar o erro num conjunto de treinamento formado por ações<br>\n","<font color='red'>(d) Aprender boas estratégias para tomar a melhor ação possível em cada situação<br></font>\n","\n","**Justificativa:** (a) é incorreta pois o objetivo não é predizer o próximo estado, mas ao invés escolher a melhor ação que maximize a recompensa, (b) agrupar ações não é o objetivo pois as ações são comumente pré-definidas e conhecidas, (c) não há erro a ser minimizado, nem conjunto de treinamento formado por ações, (d) está correta."]},{"cell_type":"markdown","metadata":{"id":"038CuS5syqqL"},"source":["---\n","\n","### Questão 2)\n","\n","Considere um agente em um ambiente, em determinado estado e possuindo um espaço de ações disponíveis. Assumindo que um algoritmo de aprendizado por reforço que já obteve aprendizado por um número de iterações arbitrário. Esse algoritmo escolherá uma ação com base:\n","\n"," <font color='red'>(a) Numa política ou uma tabela de recompensas com base em estados e ações<br></font>\n"," (b) Na predição de uma rede neural profunda usada como modelo auxiliar<br>\n"," (c) Na última observação fornecida pelo ambiente após a ação executada no instante anterior<br>\n"," (d) Em uma escolha completamente aleatória, visto que aprendizado por reforçco utiliza tentativa e erro<br>\n"," \n"," **Justificativa**: (b) é incorreta pois aprendizado por reforço não precisa necessariamente de redes neurais, (c) é incorreta pois, ainda que a ação anterior e a observação fornecida pelo ambiente sejam importantes, é o estado atual e seu potencial de recompensa que irá influenciar na escolha, (d) é incorreta pois a escolha só é aleatória quando nenhuma iteração foi executada ainda. Assim (a) é correta pois se utiliza uma política ou tabela para selecionar a próxima ação."]},{"cell_type":"markdown","metadata":{"id":"nJQ0-S3myqqL"},"source":["---\n","### Questão 3)\n","\n","Suponha que utilizamos uma rede neural para otimização de aprendizado por reforço. No cenário de Policy Learning e Value Learning, dado como entrada um estado atual, a saída da rede neural é projetada de forma a?\n","\n"," (b) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: classificação, selecionando a ação de maior valor futuro; <br>\n"," (c) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: classificação, selecionando a ação de maior valor futuro<br></font>\n"," (d) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: regressão, estimando a recompensa de cada ação disponível<br>\n"," <font color='red'>(d) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: regressão, estimando a recompensa de cada ação disponível<br></font>\n"," \n","  **Justificativa**: O aprendizado de políticas ou da função valor são as abordagens típicas. As outras estão incorretas pois, apesar de ser possível o uso de redes neurais, essas são suadas como parte das abordagens e não como principal algoritmo de treinamento, assim as técnicas: Backpropagation, SGD são incorretas; o método Deep Q-Network é um exemplo de método, mas não uma abordagem geral. O Action Critic é uma abordagem para aprendizado por reforço, mas Feed-forward é mais genérica e não caracteriza aprendizado por reforço."]},{"cell_type":"markdown","metadata":{"id":"C1sh5GgYyqqY"},"source":["---\n","\n","### Questão 4)\n","\n","Usando as bibliotecas `gym`, `gym_algorithmic` e `gym_toytext` carregue os ambientes \"MountainCarContinuous-v0\" cujo objetivo é levar um carro até o topo de um monte, \"CartPole-v1\" cujo objetivo é equilibrar um poste sobre um carro e \"Roulette-v0\" que simula um jogo de roleta.\n","\n","Como é o espaço de ações possíveis desses ambientes?\n","\n","(a) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com uma ação entre -1 e 1, Roulette: discreto com 3 ações possíveis<br>\n","<font color='red'>(b) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com duas ações possíveis, Roulette: discreto com 38 ações possíveis<br></font>\n","(c) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com duas ações possíveis, Roulette: discreto com 3 ações possíveis<br>\n","(d) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com 38 ações possíveis, Roulette: discreto com duas ações possíveis<br>\n","\n"," **Justificativa**: Ver código abaixo."]},{"cell_type":"code","metadata":{"id":"ufpeWuRXUwam","outputId":"cfc0df2f-6296-4385-cded-128f209247be"},"source":["import gym\n","import gym_algorithmic, gym_toytext\n","import random\n","\n","random.seed(1)\n","\n","env1 = gym.make(\"MountainCarContinuous-v0\")\n","env3 = gym.make(\"CartPole-v1\")\n","env4 = gym.make(\"Roulette-v0\")\n","print(\"MountainCarContinuous: \", env1.action_space)\n","print(\"CartPole: \", env3.action_space)\n","print(\"Roulette: \", env4.action_space)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["MountainCarContinuous:  Box([-1.], [1.], (1,), float32)\n","CartPole:  Discrete(2)\n","Roulette:  Discrete(38)\n"]}]},{"cell_type":"markdown","metadata":{"id":"eRbSyAXeUwaq"},"source":["---\n","\n","### Questão 5)\n","\n","Carregue o ambiente `Blackjack-v1`. Esse problema gera recompensa +1 para vitória, 0 para empate, e -1 para derrota. Inicialize o ambiente, execute 50 mil episódios (cada um até o estado terminal) e calcule a média de ações por episódio (MA), a porcentagem de vitórias (PV), de derrotas (PD) e de empates (PE), sendo vitórias (recompensa = 1), derrotas (recompensa = -1) e empates (recompensa = 0) medidas após o final de cada episódio. Arredonde o valor de MA para 2 casas decimais e considere apenas a parte inteira das porcentagens PV, PE, PD.\n","\n","Qual o resultado de MA e como se relacionam os valores obtidos de PD, PE e PV?\n","\n","<font color='red'>(a) MA está entre 1 e 2; PD > PV > PE <br></font>\n","(b) MA está entre 0 e 1; PD = PV = PE<br>\n","(c) MA está entre 0 e 1; PD > PV > PE <br>\n","(d) MA está entre 1 e 2; PD = PV = PE<br>\n","\n"," **Justificativa**: Ver código abaixo."]},{"cell_type":"code","metadata":{"id":"oTm0H0TIUwar","outputId":"90dfa7ee-c30c-4bf4-8c6b-ba5f0dc9f818"},"source":["env = gym.make(\"Blackjack-v1\")\n","\n","n_episodios_teste = 50000\n","total_epochs = 0\n","total_recs = 0\n","total_vit = 0\n","total_der = 0\n","total_emp = 0\n","\n","for i in range(n_episodios_teste):\n","    s = env.reset()\n","    epochs, rec_total_i = 0,0\n","    fim = False\n","    while not fim:\n","        a = env.action_space.sample()\n","        s, r, fim, info = env.step(a)\n","        epochs += 1\n","        rec_total_i += r\n","    \n","    if (r == 1):\n","        total_vit += 1\n","    elif (r == -1):\n","        total_der +=1\n","    elif (r == 0):\n","        total_emp +=1\n","        \n","    total_epochs += epochs\n","    total_recs += rec_total_i\n","\n","print(\"Média de ações por episódio: %.2f\" % (total_epochs/n_episodios_teste))\n","print(\"Porcentagem de vitórias: %.0f%%\" % (100*(total_vit/n_episodios_teste)))\n","print(\"Porcentagem de derrotas: %.0f%%\" % (100*(total_der/n_episodios_teste)))\n","print(\"Porcentagem de empates: %.0f%%\" % (100*(total_emp/n_episodios_teste)))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Média de ações por episódio: 1.37\n","Porcentagem de vitórias: 28%\n","Porcentagem de derrotas: 68%\n","Porcentagem de empates: 4%\n"]}]},{"cell_type":"code","metadata":{"id":"AgtagSTWUwas"},"source":[""],"execution_count":null,"outputs":[]}]}