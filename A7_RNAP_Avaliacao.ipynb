{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "**Qual** das alternativas melhor descreve o objetivo de um modelo de aprendizado por reforço?\n",
    "\n",
    "(a) Aprender a predizer o próximo estado com base em uma ação atual em determinado ambiente<br>\n",
    "(b) Aprender a agrupar ações com base na recompensa que oferecem<br>\n",
    "(c) Aprender uma política que permita obter agentes que mapeiem o ambiente de forma a minimizar o erro num conjunto de treinamento formado por ações<br>\n",
    "<font color='red'>(d) Aprender boas estratégias para tomar a melhor ação possível em cada situação</font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otimizar com objetivo de: tomar boas decisões ou aprender boas estratégias <br>\n",
    "Consequências atrasadas: decisões atuais podem impactar a longo prazo <br>\n",
    "Aprender tomando decisões e falhando por meio de recompensas, sem saber se outra ação seria melhor ou não <br>\n",
    "Generalização: uma política mapeia experiências passadas em uma ação <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Considere um agente em um ambiente, em determinado estado e possuindo um espaço de ações disponíveis. Assumindo que um algoritmo de aprendizado por reforço que já obteve aprendizado por um número de iterações arbitrário. Esse algoritmo escolherá uma ação com base:\n",
    "\n",
    "<font color='red'> (a) Numa política ou uma tabela de recompensas com base em estados e ações</font><br>\n",
    " (b) Na predição de uma rede neural profunda usada como modelo auxiliar<br>\n",
    " (c) Na última observação fornecida pelo ambiente após a ação executada no instante anterior<br>\n",
    " (d) Em uma escolha completamente aleatória, visto que aprendizado por reforçco utiliza tentativa e erro<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Política: Define o comportamento do agente\n",
    "\n",
    "Mapa do estado para a ação pode ser:<br>\n",
    "I  Determinístico: a = π(s) <br>\n",
    "II Probabilístico/estocástico: a = π(a|s) = P[a|s] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Suponha que utilizamos uma rede neural para otimização de aprendizado por reforço. No cenário de Policy Learning e Value Learning, dado como entrada um estado atual, a saída da rede neural é projetada de forma a?\n",
    "\n",
    " (a) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: classificação, selecionando a ação de maior valor futuro; <br>\n",
    " (b) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: classificação, selecionando a ação de maior valor futuro<br>\n",
    " (c) Policy: regressão, predizendo a recompensa no formato de probabilidades. Value: regressão, estimando a recompensa de cada ação disponível<br>\n",
    "<font color='red'> (d) Policy: classificação, selecionando a ação que obterá a maior recompensa. Value: regressão, estimando a recompensa de cada ação disponível </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value (Q-Learning) é Determinístico, estima a recompensa de cada ação possível e escolhe a ação de maior recompensa => regressão\n",
    "\n",
    "Policy é Probabilístico/Estocástico, seleciona a ação de maior probabilidade (maior recompensa futura), podendo ser uma ação discreta (com proba de cada ação) ou contínua (distribuição de probabilidades com média mu e dp sigma2) => classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Usando as bibliotecas `gym`, `gym_algorithmic` e `gym_toytext` carregue os ambientes \"MountainCarContinuous-v0\" cujo objetivo é levar um carro até o topo de um monte, \"CartPole-v1\" cujo objetivo é equilibrar um poste sobre um carro e \"Roulette-v0\" que simula um jogo de roleta.\n",
    "\n",
    "Como é o espaço de ações possíveis desses ambientes?\n",
    "\n",
    "(a) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com uma ação entre -1 e 1, Roulette: discreto com 3 ações possíveis<br>\n",
    "<font color='red'>(b) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com duas ações possíveis, Roulette: discreto com 38 ações possíveis</font><br>\n",
    "(c) MountainCarContinuous: contínuo com 3 ações sendo 1 negativa, uma positiva e uma tupla, CartPole: contínuo com duas ações possíveis, Roulette: discreto com 3 ações possíveis<br>\n",
    "(d) MountainCarContinuous: contínuo com uma ação com valor entre -1 e 1, CartPole: discreto com 38 ações possíveis, Roulette: discreto com duas ações possíveis<br>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install gym-algorithmic gym-legacy-toytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym-algorithmic gym-legacy-toytext\n",
    "import gym\n",
    "import gym_algorithmic, gym_toytext\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levar um carro até o topo de um monte: Box([-1.], [1.], (1,), float32)\n",
      "equilibrar um poste sobre um carro: Discrete(2)\n",
      "jogo de roleta: Discrete(38)\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make(\"MountainCarContinuous-v0\")\n",
    "print('levar um carro até o topo de um monte:', env1.action_space)\n",
    "\n",
    "env2 = gym.make(\"CartPole-v1\")\n",
    "print('equilibrar um poste sobre um carro:', env2.action_space)\n",
    "\n",
    "env3 = gym.make(\"Roulette-v0\")\n",
    "print('jogo de roleta:', env3.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"MountainCarContinuous-v0\": levar um carro até o topo de um monte:: Valores contínuos entre -1 e 1 <br> \n",
    "\"CartPole-v1\": equilibrar um poste sobre um carro:: 2 valores discretos <br>\n",
    "\"Roulette-v0\": jogo de roleta: 38 valores discretos <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v1`. Esse problema gera recompensa +1 para vitória, 0 para empate, e -1 para derrota. Inicialize o ambiente, execute 50 mil episódios (cada um até o estado terminal) e calcule a média de ações por episódio (MA), a porcentagem de vitórias (PV), de derrotas (PD) e de empates (PE), sendo vitórias (recompensa = 1), derrotas (recompensa = -1) e empates (recompensa = 0) medidas após o final de cada episódio. Arredonde o valor de MA para 2 casas decimais e considere apenas a parte inteira das porcentagens PV, PE, PD.\n",
    "\n",
    "Qual o resultado de MA e como se relacionam os valores obtidos de PD, PE e PV?\n",
    "\n",
    "(a) MA está entre 1 e 2; PD > PE > PV<br>\n",
    "(b) MA está entre 0 e 1; PD = PE = PV<br>\n",
    "(c) MA está entre 0 e 1; PD > PE > PV<br>\n",
    "(d) MA está entre 1 e 2; PD = PE = PV<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão anulada pois a resposta seria: PD > PV > PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodios_teste = 50000\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "total_vit = 0                 # total de vitorias\n",
    "total_der = 0                 # total de derrotas\n",
    "total_emp = 0                 # total de empates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio:  50000\n",
      "Total de recompensas -19586\n",
      "Média de recompensas por episódio -0.39\n",
      "Total de ações 68928\n",
      "MA - Média de ações por episódio 1.38\n",
      "Número de Vitórias 14168 PV 28\n",
      "Número de Derrotas 33754 PD 68\n",
      "Número de Empates 2078 PE 4\n"
     ]
    }
   ],
   "source": [
    "# historico\n",
    "\n",
    "# random.seed(1)\n",
    "\n",
    "n_episodios_teste = 50000\n",
    "total_epochs = 0              # total de passos\n",
    "total_recs = 0                # total de recompensas\n",
    "total_vit = 0                 # total de vitorias\n",
    "total_der = 0                 # total de derrotas\n",
    "total_emp = 0                 # total de empates\n",
    "\n",
    "# episodios\n",
    "for t in range(1, n_episodios_teste+1):\n",
    "    s = env.reset()\n",
    "    epochs = 0\n",
    "    fim = False\n",
    "\n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "        a = env.action_space.sample()\n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        s = s_n\n",
    "        epochs += 1\n",
    "        total_recs += r      \n",
    "\n",
    "    total_epochs += epochs\n",
    "    \n",
    "    if r>0:\n",
    "        total_vit += 1\n",
    "    elif r<0:\n",
    "        total_der += 1\n",
    "    else:\n",
    "        total_emp += 1\n",
    "    \n",
    "    if (t % 250 == 0):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episódio: \", t)    \n",
    "\n",
    "print(\"Total de recompensas %.d\" % (total_recs))        \n",
    "print(\"Média de recompensas por episódio %.2f\" % (total_recs/(n_episodios_teste+1)))\n",
    "print(\"Total de ações %.d\" % (total_epochs))\n",
    "print(\"MA - Média de ações por episódio %.2f\" % (total_epochs/(n_episodios_teste+1)))\n",
    "print(\"Número de Vitórias %d PV %.0f\" % (total_vit, (total_vit/(n_episodios_teste+1))*100))\n",
    "print(\"Número de Derrotas %d PD %.0f\" % (total_der, (total_der/(n_episodios_teste+1))*100))\n",
    "print(\"Número de Empates %d PE %.0f\" % (total_emp, (total_emp/(n_episodios_teste+1))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
